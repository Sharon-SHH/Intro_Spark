# ===== Root logger =====
status = warn
name = SparkLog4j2
rootLogger.level = warn
rootLogger.appenderRefs = console
rootLogger.appenderRef.console.ref = Console

# ===== Console appender =====
appender.console.type = Console
appender.console.name = Console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# ===== Reduce Spark internal noise =====
logger.spark.name = org.apache.spark
logger.spark.level = warn
logger.spark_project.name = org.spark_project
logger.spark_project.level = warn
logger.hadoop.name = org.apache.hadoop
logger.hadoop.level = warn
logger.netty.name = io.netty
logger.netty.level = warn
logger.zk.name = org.apache.zookeeper
logger.zk.level = warn

# ===== Your application package =====
# 对应 logger.py 里的 root_class = "guru.learningjournal.spark.examples"
logger.app.name = guru.learningjournal.spark.examples
logger.app.level = info
logger.app.additivity = false
logger.app.appenderRefs = console, file
logger.app.appenderRef.console.ref = Console
logger.app.appenderRef.file.ref = RollingFile
# ===== Rolling file appender (optional) =====
appender.file.type = RollingFile
appender.file.name = RollingFile
appender.file.fileName = logs/app.log
appender.file.filePattern = logs/app-%d{yyyy-MM-dd}-%i.log.gz
appender.file.layout.type = PatternLayout
appender.file.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
appender.file.policies.type = Policies
appender.file.policies.time.type = TimeBasedTriggeringPolicy
appender.file.policies.time.interval = 1
appender.file.policies.time.modulate = true
appender.file.policies.size.type = SizeBasedTriggeringPolicy
appender.file.policies.size.size = 20MB